# âš›ï¸ QuantumBot â€” RAG + LangGraph Agent + Memory

A quantum computing chatbot powered by LangChain, LangGraph, Groq LLM, 
and Retrieval Augmented Generation (RAG).

## ğŸš€ Live Demo
[QuantumBot on Streamlit Cloud](https://langchainprojects-fqthc6ddbmups2oxsnvjpk.streamlit.app/)

## ğŸ§  Features

- **RAG** â€” searches IBM Quantum documentation for accurate answers
- **LangGraph Agent** â€” intelligently decides which tool to use
- **3 Knowledge Sources** â€” IBM Quantum Docs + ArXiv + Wikipedia
- **4 Memory Types** â€” Buffer, Summary, Buffer Window, Entity
- **Streaming Responses** â€” word by word like ChatGPT
- **Chat History** â€” full conversation displayed on screen

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| LLM | Groq (qwen/qwen3-32b) |
| Agent | LangGraph ReAct Agent |
| Vector Store | FAISS |
| Embeddings | HuggingFace (all-MiniLM-L6-v2) |
| RAG Source | IBM Quantum Docs |
| External Tools | ArXiv + Wikipedia |
| UI | Streamlit |

## ğŸ“ Project Structure
```
QuantumBot/
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env.example        # Environment variables template
â””â”€â”€ .gitignore          # Git ignore rules
```

## âš™ï¸ Setup Locally

**1. Clone the repo**
```bash
git clone https://github.com/vamsi-krishna108/langchainprojects.git
cd langchainprojects/QuantumBot
```

**2. Create conda environment**
```bash
conda create -n langchain_env python=3.11 -y
conda activate langchain_env
```

**3. Install dependencies**
```bash
pip install -r requirements.txt
```

**4. Add API keys**
```bash
cp .env.example .env
# Edit .env and add your keys
```

**.env file:**
```
GROQ_API_KEY=your_groq_api_key_here
LANGCHAIN_API_KEY=your_langchain_api_key_here
```

**5. Run**
```bash
streamlit run app.py
```

## ğŸ”‘ Get API Keys

| Key | Link |
|---|---|
| Groq API Key | https://console.groq.com |
| LangChain API Key | https://smith.langchain.com |

## ğŸ§© How It Works
```
User Question
      â†“
LangGraph Agent thinks (ReAct loop)
      â†“
Picks best tool:
  "What is a qubit?"      â†’ IBM Quantum Docs
  "Latest research?"      â†’ ArXiv
  "Simple definition?"    â†’ Wikipedia
      â†“
Retrieves relevant context
      â†“
Groq LLM generates answer
      â†“
Streams response word by word
      â†“
Saves to memory + chat history
```

## ğŸ’¾ Memory Types

| Type | Description | Best For |
|---|---|---|
| Buffer | Stores full conversation | Short chats |
| Summary | Compresses old messages | Long chats |
| Buffer Window | Keeps last 3 exchanges | Medium chats |
| Entity | Tracks specific topics | Fact tracking |

## ğŸš€ Deploy on Streamlit Cloud

1. Push code to GitHub
2. Go to [streamlit.io/cloud](https://streamlit.io/cloud)
3. Connect your GitHub repo
4. Add secrets in Settings â†’ Secrets:
```toml
GROQ_API_KEY = "your_key_here"
LANGCHAIN_API_KEY = "your_key_here"
```
5. Deploy!

## ğŸ“š What I Learned Building This

- RAG pipeline with FAISS vector store
- LangGraph ReAct agent with multiple tools
- LangChain memory types and differences
- Streamlit session state management
- Streaming responses
- Deploying to Streamlit Cloud
- Managing API keys securely

## ğŸ¤ Connect

Built by [Vamsi Krishna](https://github.com/vamsi-krishna108)